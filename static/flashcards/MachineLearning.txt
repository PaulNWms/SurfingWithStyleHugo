Categories of ML systems
supervised vs. unsupervised<br />
online vs. batch<br />
instance-based vs. model-based

The training data you feed to the algorithm includes labels.
supervised learning

k-Nearest Neighbors is an algorithm for _____.
supervised learning

Linear Regression is an algorithm for _____.
supervised learning

Logistic Regression is an algorithm for _____.
supervised learning

Support Vector Machines (SVMs) is an algorithm for _____.
supervised learning

Decision Trees and Random Forests is an algorithm for _____.
supervised learning

(most) Neural networks is an algorithm for _____.
supervised learning

The training data you feed to the algorithm does not include labels.
unsupervised learning

Clustering is a task of ______.
unsupervised learning

Visualization is a task of _____.
unsupervised learning

Dimensionality reduction and feature extraction are a tasks of _____.
unsupervised learning

Anomaly detection is a task of _____.
unsupervised learning

Association rule learning is a type of ______.
unsupervised learning

A deep belief network (DBN) is a type of _____.
semi-supervised learning

Deep belief networks (DBNs) are based on _____.
restricted Boltzman machines

A restricted Boltzman machine is a type of _____.
unsupervised learning

k-Means is an algorithm for _____.
clustering

Hierarchical Cluster Analysis (HCA) is an algorithm for _____.
clustering

Expectation Maximization is an algorithm for _____.
clustering

Principal Component Analysis (PCA) is an algorithm for _____.
visualization and dimensionality reduction

Kernel PCA is an algorithm for _____.
visualization and dimensionality reduction

Locally-Linear Embedding (LLE) is an algorithm for _____.
visualization and dimensionality reduction

t-distributed Stochastic Neighbor Embedding (t-SNE) is an algorithm for _____.
visualization and dimensionality reduction

Apriori is an algorithm for _____.
association rule learning

Eclat is an algorithm for _____.
association rule learning

A similarity measure to existing data to make predictions in _____.
instance-based learning

Linear regression is a type of _____.
model-based learning

Feature selection is a task of _____.
feature engineering

Feature extraction and dimensionality reduction are tasks of _____.
feature engineering

Creating new features by gathering new data is a task of _____.
feature engineering

The data is split into a _____ and a ______.
training set<br />
test set

The training set is split into a _____ and a _____.
training set<br />
validation set

An alternative to splitting the training set and the validation set is _____.
cross-validation

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">RMSE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ—</mtext><mo>,</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ğ±</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><msup><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\text{RMSE}(\textbf{X},h)=\sqrt{\frac{1}{m}\sum_{i=1}^{m}(h(\textbf{x}^{(i)}) - y^{(i)})^{2}}</annotation></semantics></math>
Root Mean Square Error (RMSE)<br />
Euclidian norm<br />
â„“â‚‚ norm

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">MAE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ—</mtext><mo>,</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="true" form="prefix">|</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ğ±</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆ’</mo><msup><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">\text{MAE}(\textbf{X},h)=\frac{1}{m}\sum_{i=1}^{m}|h(\textbf{x}^{(i)}-y^{(i)})|</annotation></semantics></math>
Mean Absolute Error (MAE)<br />
Manhattan norm<br />
â„“â‚ norm

Python: join paths
os.path.join()

Python: download file
urllib.request.urlretrieve()

pandas: read CSV into DataFrame
pandas.read_csv()

pandas: list top 5 rows
df.head()

pandas: get quick description of data
df.info()

pandas: get summary of numerical data
df.describe()

Jupyter: magic command to enable Matplotlib
%matplotlib inline

ScikitLearn: put aside test set
train_set, test_set = train_test_split()

NumPy: create array with shuffled indexes
np.random.permutation(len(data))

pandas: segment and sort data values into bins
pandas.cut

ScikitLearn: create a stratified test set
StratifiedShuffleSplit

pandas: create a correlation matrix
df.corr(numeric_only=True)

pandas: create scatter plots
scatter_matrix()

pandas: remove rows with a missing value
data.dropna()

pandas: remove columns
data.drop()

pandas: fill in missing data
data[column].fillna(median, inplace=True)

ScikitLearn: fill in missing values
Imputer

pandas: map categorical data to integers
col.factorize()

ScikitLearn: one-hot encode categorical data
OneHotEncoder

ScikitLearn: min-max scale numerical data
MinMaxScaler

pandas: perform a log operation on a column
col.apply(np.log)

ScikitLearn: standardize numerical data
StandardScaler

ScikitLearn: train pipeline
pipeline.fit_transform()

ScikitLearn: save model
joblib

ScikitLearn: grid search
GridSearchCV

ScikitLearn: randomized search
RandomSearchCV

ScikitLearn: run trained pipeline
pipeline.transform()

predict values
regression

predict classes
classification

ScikitLearn: stochastic gradient descent (SGD)
SGDClassifier

ScikitLearn: confusion matrix
confusion_matrix()

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{TP}{TP+FP}</annotation></semantics></math>
precision

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{TP}{TP+FN}</annotation></semantics></math>
recall<br />
TPR<br />
sensitivity

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mn>2</mn><mrow><mfrac><mn>1</mn><mtext mathvariant="normal">precision</mtext></mfrac><mo>+</mo><mfrac><mn>1</mn><mtext mathvariant="normal">recall</mtext></mfrac></mrow></mfrac><annotation encoding="application/x-tex">\frac{2}{\frac{1}{\text{precision}} + \frac{1}{\text{recall}}}</annotation></semantics></math>
Fâ‚

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mtext mathvariant="normal">precision</mtext><mo>Ã—</mo><mtext mathvariant="normal">recall</mtext></mrow><mrow><mrow><mtext mathvariant="normal">precision </mtext><mspace width="0.333em"></mspace></mrow><mo>+</mo><mtext mathvariant="normal">recall</mtext></mrow></mfrac><annotation encoding="application/x-tex">\frac{\text{precision}\times \text{recall}}{\text{precision }+ \text{recall}}</annotation></semantics></math>
Fâ‚

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mfrac><mrow><mi>F</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow><mn>2</mn></mfrac></mrow></mfrac><annotation encoding="application/x-tex">\frac{TP}{TP+\frac{FN+FP}{2}}</annotation></semantics></math>
Fâ‚

ScikitLearn: cross-validation score
cross_val_score()

ScikitLearn: cross-validate on test set
cross_val_predict()

ScikitLearn: get data to plot precision &amp; recall, also precision vs. recall
precision_recall_curve()

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{TN}{TN+FP}</annotation></semantics></math>
TNR<br />
specificity

1 - TNR
FPR

The receiver operating characteric (ROC) curve plots
sensitivity vs. 1 - specificity

ScikitLearn: get data to plot receiver operating characteric (ROC) curve
roc_curve()

NumPy: create array of random integers
np.random.randint()

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><msub><mi>Î¸</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Î¸</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Î¸</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>â‹¯</mi><mo>+</mo><msub><mi>Î¸</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\hat{y}=\theta_0+\theta_1x_1+\theta_2x_2+\cdots +\theta_nx_n</annotation></semantics></math>
Linear Regression model prediction

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><msub><mi>h</mi><mi>Î¸</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ±</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>Î¸</mi><mi>T</mi></msup><mo>â‹…</mo><mtext mathvariant="bold">ğ±</mtext></mrow><annotation encoding="application/x-tex">\hat{y}=h_{\theta}(\textbf{x})=\theta^T\cdot \textbf{x}</annotation></semantics></math>
vectorized Linear Regression model prediction

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>S</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ—</mtext><mo>,</mo><msub><mi>h</mi><mi>Î¸</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Î¸</mi><mi>T</mi></msup><mo>â‹…</mo><msup><mtext mathvariant="bold">ğ±</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆ’</mo><msup><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">MSE(\textbf{X}, h_{\theta})=\frac{1}{m}\sum_{i=1}^{m}\left(\theta^T\cdot \textbf{x}^{(i)}-y^{(i)}  \right)^2</annotation></semantics></math>
MSE cost function for a Linear Regression model

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Î¸</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ğ—</mtext><mi>T</mi></msup><mo>â‹…</mo><mtext mathvariant="bold">ğ—</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><mo>â‹…</mo><msup><mtext mathvariant="bold">ğ—</mtext><mi>T</mi></msup><mo>â‹…</mo><mtext mathvariant="bold">ğ²</mtext></mrow><annotation encoding="application/x-tex">\hat{\theta}=(\textbf{X}^T\cdot \textbf{X})^{-1}\cdot \textbf{X}^T\cdot \textbf{y}</annotation></semantics></math>
normal equation

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>Î´</mi><mrow><mi>Î´</mi><msub><mi>Î¸</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>2</mn><mi>m</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Î¸</mi><mi>T</mi></msup><mo>â‹…</mo><msup><mtext mathvariant="bold">ğ±</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆ’</mo><msup><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mi>x</mi><mi>j</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\frac{\delta}{\delta\theta_j}=\frac{2}{m}\sum_{i=1}^{m}\left(\theta^T\cdot \textbf{x}^{(i)}-y^{(i)}\right)x_j^{(i)}</annotation></semantics></math>
partial derivatives of the cost function

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>M</mi><mi>S</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mi>Î´</mi><mrow><mi>Î´</mi><msub><mi>Î¸</mi><mn>0</mn></msub></mrow></mfrac><mi>M</mi><mi>S</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mi>Î´</mi><mrow><mi>Î´</mi><msub><mi>Î¸</mi><mn>1</mn></msub></mrow></mfrac><mi>M</mi><mi>S</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>â‹®</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mi>Î´</mi><mrow><mi>Î´</mi><msub><mi>Î¸</mi><mi>n</mi></msub></mrow></mfrac><mi>M</mi><mi>S</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>2</mn><mi>m</mi></mfrac><msup><mtext mathvariant="bold">ğ—</mtext><mi>T</mi></msup><mo>â‹…</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ—</mtext><mo>â‹…</mo><mi>Î¸</mi><mo>âˆ’</mo><mtext mathvariant="bold">ğ²</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla _{\theta}MSE(\theta)=\begin{pmatrix} \frac{\delta}{\delta\theta_0}MSE(\theta) \\\frac{\delta}{\delta\theta_1}MSE(\theta)  \\\vdots \\\frac{\delta}{\delta\theta_n}MSE(\theta)\end{pmatrix}=\frac{2}{m}\textbf{X}^T\cdot (\textbf{X}\cdot \theta-\textbf{y})</annotation></semantics></math>
gradient vector of the cost function

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Î¸</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">next step</mtext><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>Î¸</mi><mo>âˆ’</mo><mi>Î·</mi><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>M</mi><mi>S</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta^{(\text{next step})}=\theta - \eta\nabla _{\theta}MSE(\theta)</annotation></semantics></math>
gradient descent step

ScikitLearn: Stochastic GD
SGDRegressor

ScikitLearn: Mini-batch GD
SGDRegressor

ScikitLearn: polynomial regression
PolynomialFeatures

Generalization error due to wrong assumptions is called _____.
bias

A model's excessive sensitivity to small variations in training data is called _____.
variance

Noisiness in the data causes _____.
irreducible error

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">MSE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>Î±</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>Î¸</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">J(\theta)=\text{MSE}(\theta)+\alpha\frac{1}{2}\sum_{i=1}^{n}\theta_i^2</annotation></semantics></math>
ridge regression cost function

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Î¸</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mtext mathvariant="bold">ğ—</mtext><mi>T</mi></msup><mo>â‹…</mo><mtext mathvariant="bold">ğ—</mtext><mo>+</mo><mi>Î±</mi><mtext mathvariant="bold">ğ€</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><mo>â‹…</mo><msup><mtext mathvariant="bold">ğ—</mtext><mi>T</mi></msup><mo>â‹…</mo><mtext mathvariant="bold">ğ²</mtext></mrow><annotation encoding="application/x-tex">\hat\theta=\left(\textbf{X}^T\cdot \textbf{X}+\alpha \textbf{A}\right)^{-1}\cdot \textbf{X}^T\cdot \textbf{y}</annotation></semantics></math>
ridge regression closed-form solution

ScikitLearn: ridge regression
Ridge<br />
SGDRegressor(penalty="l2")

LASSO regression
Least Absolute Shrinkage and Selection Operator regression

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">MSE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>Î±</mi><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>Î¸</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">J(\theta)=\text{MSE}(\theta)+\alpha\sum_{i=1}^{n}\left|\theta_i\right|</annotation></semantics></math>
LASSO regression cost function

ScikitLearn: LASSO regression
Lasso<br />
SGDRegressor(penalty="l1")

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">MSE</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>r</mi><mi>Î±</mi><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>Î¸</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mfrac><mrow><mn>1</mn><mo>âˆ’</mo><mi>r</mi></mrow><mn>2</mn></mfrac><mi>Î±</mi><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>Î¸</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">J(\theta)=\text{MSE}(\theta)+r\alpha\sum_{i=1}^{n}\left|\theta_i\right|+\frac{1-r}{2}\alpha\sum_{i=1}^{n}\theta_i^2</annotation></semantics></math>
Elastic Net cost function

ScikitLearn: Elastic Net
ElasticNet

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>p</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><msub><mi>h</mi><mi>Î¸</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ±</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>Ïƒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Î¸</mi><mi>T</mi></msup><mo>â‹…</mo><mtext mathvariant="bold">ğ±</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{p}=h_\theta(\textbf{x})=\sigma(\theta^T\cdot \textbf{x})</annotation></semantics></math>
Logistic Regression model estimated probability

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ïƒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext mathvariant="normal">exp</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(t)=\frac{1}{1+\text{exp}(-t)}</annotation></semantics></math>
logistic function

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>âˆ’</mi><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mover><mi>l</mi><mo accent="true">Ì‚</mo></mover><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msup><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msup><mover><mi>p</mi><mo accent="true">Ì‚</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left[y^{(i)}\hat log\left(p^{(i)}\right)+\left(1-y^{(i)}\right)log\left( 1-\hat p^{(i)} \right)\right]</annotation></semantics></math>
Logistic Regression cost function (log loss)

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>Î´</mi><mrow><mi>Î´</mi><msub><mi>Î¸</mi><mi>j</mi></msub></mrow></mfrac><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ïƒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Î¸</mi><mi>T</mi></msup><mo>â‹…</mo><msup><mtext mathvariant="bold">ğ±</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><msup><mi>y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mi>x</mi><mi>j</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\frac{\delta}{\delta\theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}\left(\sigma\left(\theta^T\cdot \textbf{x}^{(i)}\right)-y^{(i)}\right)x_j^{(i)}</annotation></semantics></math>
logistic cost function partial derivatives

ScikitLearn: logistic regression
LogisticRegression

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ±</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Î¸</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>â‹…</mo><mtext mathvariant="bold">ğ±</mtext></mrow><annotation encoding="application/x-tex">s_k(\textbf{x})=\left(\theta^{(k)} \right)^T\cdot \textbf{x}</annotation></semantics></math>
SoftMax score for class k

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>p</mi><mo accent="true">Ì‚</mo></mover><mi>k</mi></msub><mo>=</mo><mi>Ïƒ</mi><msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ¬</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ±</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>k</mi></msub><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>s</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ±</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>s</mi><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="bold">ğ±</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\hat{p}_k=\sigma(\textbf{s}(\textbf{x}))_k=\frac{exp(s_k(\textbf{x}))}{\sum_{j=1}^{K}exp(s_j(\textbf{x}))}</annotation></semantics></math>
SoftMax function

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î˜</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>y</mi><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mover><mi>p</mi><mo accent="true">Ì‚</mo></mover><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\Theta)=\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log\left(\hat{p}_k^{(i)}\right)</annotation></semantics></math>
cross entropy cost function

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>âˆ‡</mi><msup><mi>Î¸</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></msub><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Î˜</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mover><mi>p</mi><mo accent="true">Ì‚</mo></mover><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>âˆ’</mo><msubsup><mi>y</mi><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><msup><mtext mathvariant="bold">ğ±</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\nabla_{\theta^{(k)}}J(\Theta)=\frac{1}{m}\sum_{i=1}^{m}\left(\hat{p}_k^{(i)}-y_k^{(i)}\right)\textbf{x}^{(i)}</annotation></semantics></math>
cross entropy gradient vector for class k

ScikitLearn: linear support vector machine
LinearSVC<br />
LinearSVR

ScikitLearn: nonlinear support vector machine
pipeline w. PolynomialFeatures &amp; LinearSVC<br />
SVC<br />
SVR

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ï•</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ±</mi><mo>,</mo><mtext mathvariant="italic">ğ‘™</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mi>Î³</mi><msup><mrow><mo stretchy="true" form="prefix">âˆ¥</mo><mi>ğ±</mi><mo>âˆ’</mo><mtext mathvariant="italic">ğ‘™</mtext><mo stretchy="true" form="postfix">âˆ¥</mo></mrow><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \phi (\mathbf{x}, \textit{l})=exp\left(-\gamma \left\| \mathbf{x}-\textit{l} \right\|^2\right)</annotation></semantics></math>
Gaussian radial basis function (RBF)

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>âˆ’</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">max(0, 1-t)</annotation></semantics></math>
hinge loss function

ScikitLearn: decision tree
DecisionTreeClassifier<br />
DecisionTreeRegressor

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>âˆ’</mo><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">G_i=1-\sum_{k=1}^{n}p_{i,k}^2</annotation></semantics></math>
Gini impurity

ScikitLearn: decision tree algorithm
Classification and Regression Tree (CART)

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>,</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><msub><mi>m</mi><mtext mathvariant="normal">left</mtext></msub><mi>m</mi></mfrac><msub><mi>G</mi><mtext mathvariant="normal">left</mtext></msub><mo>+</mo><mfrac><msub><mi>m</mi><mtext mathvariant="normal">right</mtext></msub><mi>m</mi></mfrac><msub><mi>G</mi><mtext mathvariant="normal">right</mtext></msub></mrow><annotation encoding="application/x-tex">J(k,t_k)=\frac{m_{\text{left}}}{m}G_{\text{left}}+\frac{m_{\text{right}}}{m}G_{\text{right}}</annotation></semantics></math>
CART cost function for classification

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>i</mi></msub><mo>=</mo><mi>âˆ’</mi><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mtext mathvariant="normal">log</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>p</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">H_i=-\sum_{k=1}^{n}p_{i,k}\text{log}(p_{i,k})</annotation></semantics></math>
entropy

<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>,</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><msub><mi>m</mi><mtext mathvariant="normal">left</mtext></msub><mi>m</mi></mfrac><msub><mtext mathvariant="normal">MSE</mtext><mtext mathvariant="normal">left</mtext></msub><mo>+</mo><mfrac><msub><mi>m</mi><mtext mathvariant="normal">right</mtext></msub><mi>m</mi></mfrac><msub><mtext mathvariant="normal">MSE</mtext><mtext mathvariant="normal">right</mtext></msub></mrow><annotation encoding="application/x-tex">J(k,t_k)=\frac{m_{\text{left}}}{m}\text{MSE}_{\text{left}}+\frac{m_{\text{right}}}{m}\text{MSE}_{\text{right}}</annotation></semantics></math>
CART cost function for regression

ScikitLearn: voting classifier
VotingClassifier

sampling is performed with replacement
bagging

sampling is performed without replacement
pasting

ScikitLearn: bagging classifier
BaggingClassifier

ScikitLearn: pasting classifier
BaggingClassifier(bootstrap=False)

ScikitLearn: use the out-of-bag set for validation
BaggingClassifier(oob_score=True)

ScikitLearn: bagging a decision tree
RandomForestClassifier<br />
BaggingClassifier w/ a DecisionTreeClassifier

ScikitLearn: make forests of extremely random trees
ExtraTreeClassifier<br />
ExtraTreeRegressor

creating a strong learner from weak learners
boosting

Sequentially add predictors to an ensemble, each one correcting its predecessor by weighting the misclassified instances.
AdaBoost

ScikitLearn: AdaBoost classifier
AdaBoostClassifier

Sequentially add predictors to an ensemble, trying to fit the new predictor to residual errors of the previous predictor.
Gradient Boost

ScikitLearn: Gradient Boost regressor
GradiengBoostingRegressor

training a model to aggregate predictions of predictors in an ensemble
stacking

ScikitLearn: stacking
not supported

ScikitLearn: PCA dimensionality reduction
PCA<br />
IncrementalPCA<br />
KernelPCA

ScikitLearn: randomized PCA
PCA(svd_solver="randomized")

ScikitLearn: LLE dimensionality reduction
LocallyLinearEmbedding

TensorFlow: variable
tf.Variable(3, name="x")

TensorFlow: variable initialization node
init = tf.global_variable_initializer()

TensorFlow: initialize variables
sess.run(init)

TensorFlow: session
with tf.Session() as sess:

TensorFlow: interactive
sess = tf.InteractiveSession()

TensorFlow: default graph
tf.get_default_graph()

TensorFlow: new graph
graph = tf.graph()<br />
with graph.as_default:

TensorFlow: constant
X = tf.constant(...)

TensorFlow: transpose matrix
XT = tf.transpose(X)

TensorFlow: matrix multiply
theta = tf.matmul(...)

TensorFlow: evaluate
theta_value = theta.eval()

TensorFlow: update a variable
training_op = tf.assign(...)

TensorFlow: autodiff
gradients = tf.gradients(mse, [theta])[0]

TensorFlow: gradient descent
optimizer = tf.train.GradientDescentOptimizer(...)

TensorFlow: momentum
optimizer = tf.train.MomentumOptimizer(...)

TensorFlow: specify data to be passed in
A = tf.placeholder(tf.float32, shape=(None, 3))

TensorFlow: pass in data
B_val = B.eval(feed_dict={A: [[1, 2, 3]]})

TensorFlow: create saver node
saver = tf.train.Saver()

TensorFlow: save model
save_path = saver.save(sess, "/tmp/my_model.ckpt")

TensorFlow: restore model
saver.restore(sess, "/tmp/my_model.ckpt")

TensorFlow: restore graph
saver = tf.train.import_meta_graph(sess, "/tmp/my_model.ckpt.meta")

TensorFlow: name scope
with tf.name_scope("loss") as scope:

TensorFlow: variable scope
with tf.variable_scope("relu", reuse=True):

Machine Learning is great for
- complex problems for which we have no algorithmic solution<br />
- to replace long lists of hand-tuned rules<br />
- to build systems that adapt to fluctuating environments<br />
- data mining and visualization
